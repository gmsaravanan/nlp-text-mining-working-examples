{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keyword Extraction with TF-IDF and SKlearn.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "GzQvIhfxew9k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extracting Important Keywords from Text with TF-IDF and Python's Scikit-Learn \n",
        "\n",
        "Back in 2006, when I had to use TF-IDF for keyword extraction in Java, I ended up writing all of the code from scratch as Data Science nor GitHub were a thing back then and libraries were just limited. The world is much different today. You have several [libraries](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) and [open-source code on Github](https://github.com/topics/tf-idf?o=desc&s=forks) that provide a decent implementation of TF-IDF. If you don't need a lot of control over how the TF-IDF math is computed then I would highly recommend re-using libraries from known packages such as [Spark's MLLib](https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html) or [Python's scikit-learn](http://scikit-learn.org/stable/). \n",
        "\n",
        "The one problem that I noticed with these libraries is that they are meant as a pre-step for other tasks like clustering, topic modeling and text classification. [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) can actually be used to extract important keywords from a document to get a sense of what characterizes a document. For example, if you are dealing with wikipedia articles, you can use tf-idf to extract words that are unique to a given article. These keywords can be used as a very simple summary of the document, it can be used for text-analytics (when we look at these keywords in aggregate), as candidate labels for a document and more. \n",
        "\n",
        "In this article, I will show you how you can use scikit-learn to extract top keywords for a given document using its tf-idf modules. We will specifically do this on a stackoverflow dataset. "
      ]
    },
    {
      "metadata": {
        "id": "52exyfMYew9n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "Since we used some pretty clean user reviews in some of my previous tutorials, in  this example, we will be using a Stackoverflow dataset which is slightly noisier and simulates what you could be dealing with in real life. You can find this dataset in [my tutorial repo](https://github.com/kavgan/data-science-tutorials/tree/master/tf-idf/data). Notice that there are two files, the larger file with (20,000 posts)[https://github.com/kavgan/data-science-tutorials/tree/master/tf-idf/data] is used to compute the Inverse Document Frequency (IDF) and the smaller file with [500 posts](https://github.com/kavgan/data-science-tutorials/tree/master/tf-idf/data) would be used as a test set for us to extract keywords from. This dataset is based on the publicly available [Stackoverflow dump on Google's Big Query](https://cloud.google.com/bigquery/public-data/stackoverflow).\n",
        "\n",
        "Let's take a peek at our dataset. The code below reads a one per line json string from `data/stackoverflow-data-idf.json` into a pandas data frame and prints out its schema and total number of posts. Here, `lines=True` simply means we are treating each line in the text file as a separate json string. With this, the json in line 1 is not related to the json in line 2."
      ]
    },
    {
      "metadata": {
        "id": "sTniwJ7OhSyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "585b0558-766e-4fe5-dce5-1134314804bc"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BTu75ULNew9o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "e65bf1ef-f566-4946-a33b-1fe4ebefec42"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read json into a dataframe\n",
        "df_idf=pd.read_json(\"/content/gdrive/My Drive/DataScience/data/data/stackoverflow-data-idf.json\",lines=True)\n",
        "\n",
        "# print schema\n",
        "print(\"Schema:\\n\\n\",df_idf.dtypes)\n",
        "print(\"Number of questions,columns=\",df_idf.shape)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Schema:\n",
            "\n",
            " accepted_answer_id          float64\n",
            "answer_count                  int64\n",
            "body                         object\n",
            "comment_count                 int64\n",
            "community_owned_date         object\n",
            "creation_date                object\n",
            "favorite_count              float64\n",
            "id                            int64\n",
            "last_activity_date           object\n",
            "last_edit_date               object\n",
            "last_editor_display_name     object\n",
            "last_editor_user_id         float64\n",
            "owner_display_name           object\n",
            "owner_user_id               float64\n",
            "post_type_id                  int64\n",
            "score                         int64\n",
            "tags                         object\n",
            "title                        object\n",
            "view_count                    int64\n",
            "dtype: object\n",
            "Number of questions,columns= (20000, 19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_Xet1gRXew9u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Take note that this stackoverflow dataset contains 19 fields including post title, body, tags, dates and other metadata which we don't quite need for this tutorial. What we are mostly interested in for this tutorial is the `body` and `title` which is our source of text. We will now create a field that combines both body and title so we have it in one field. We will also print the second `text` entry in our new field just to see what the text looks like."
      ]
    },
    {
      "metadata": {
        "id": "H3SH9xMNiYr9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2a81899e-edf2-4978-992b-4ee92e0c84f7"
      },
      "cell_type": "code",
      "source": [
        "df_idf.columns"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['accepted_answer_id', 'answer_count', 'body', 'comment_count',\n",
              "       'community_owned_date', 'creation_date', 'favorite_count', 'id',\n",
              "       'last_activity_date', 'last_edit_date', 'last_editor_display_name',\n",
              "       'last_editor_user_id', 'owner_display_name', 'owner_user_id',\n",
              "       'post_type_id', 'score', 'tags', 'title', 'view_count'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "mS_bBP5Jew9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4e996937-d40e-4ae6-f807-cbe897ca2cac"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "def pre_process(text):\n",
        "    \n",
        "    # lowercase\n",
        "    text=text.lower()\n",
        "    \n",
        "    #remove tags\n",
        "    text=re.sub(\"</?.*?>\",\" <> \",text)\n",
        "    \n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "df_idf['text'] = df_idf['title'] + df_idf['body']\n",
        "df_idf['text'] = df_idf['text'].apply(lambda x:pre_process(x))\n",
        "\n",
        "#show the first 'text'\n",
        "df_idf['text'][2]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gradle command line i m trying to run a shell script with gradle i currently have something like this def test project tasks create test exec commandline bash c bash c my file dir script sh the problem is that i cannot run this script because i have spaces in my dir name i have tried everything e g commandline bash c bash c my file dir script sh tokenize commandline bash c bash c my file dir script sh commandline bash c new stringbuilder append bash append c my file dir script sh commandline bash c bash c my file dir script sh file dir file c my file dir script sh commandline bash c bash dir getabsolutepath im using windows bit and if i use a path without spaces the script runs perfectly therefore the only issue as i can see is how gradle handles spaces '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "ZXcdxa6aew9y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Hmm, doesn't look very pretty with all the html in there, but that's the point. Even in such a mess we can extract some great stuff out of this. While you can eliminate all code from the text, we will keep the code sections for this tutorial for the sake of simplicity.  "
      ]
    },
    {
      "metadata": {
        "id": "GXFOfVX6ew9z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating the IDF\n",
        "\n",
        "### CountVectorizer to create a vocabulary and generate word counts\n",
        "The next step is to start the counting process. We can use the CountVectorizer to create a vocabulary from all the text in our `df_idf['text']` and generate counts for each row in `df_idf['text']`. The result of the last two lines is a sparse matrix representation of the counts, meaning each column represents a word in the vocabulary and each row represents the document in our dataset where the values are the word counts. Note that with this representation, counts of some words could be 0 if the word did not appear in the corresponding document."
      ]
    },
    {
      "metadata": {
        "id": "wJ4rP3Maew9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "16d53500-ac11-429b-abff-ac70fab88d37"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "\n",
        "def get_stop_words(stop_file_path):\n",
        "    \"\"\"load stop words \"\"\"\n",
        "    \n",
        "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
        "        stopwords = f.readlines()\n",
        "        stop_set = set(m.strip() for m in stopwords)\n",
        "        return frozenset(stop_set)\n",
        "\n",
        "#load a set of stop words\n",
        "stopwords=get_stop_words(\"/content/gdrive/My Drive/DataScience/data/resources/stopwords.txt\")\n",
        "\n",
        "#get the text column \n",
        "docs=df_idf['text'].tolist()\n",
        "\n",
        "#create a vocabulary of words, \n",
        "#ignore words that appear in 85% of documents, \n",
        "#eliminate stop words\n",
        "cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
        "word_count_vector=cv.fit_transform(docs)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "81HOJQZ6ew91",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's check the shape of the resulting vector. Notice that the shape below is `(20000,149391)` because we have 20,000 documents in our dataset (the rows) and the vocabulary size is `149391` meaning we have `149391` unique words (the columns) in our dataset minus the stopwords. In some of the text mining applications, such as clustering and text classification we limit the size of the vocabulary. It's really easy to do this by setting `max_features=vocab_size` when instantiating CountVectorizer."
      ]
    },
    {
      "metadata": {
        "id": "wetrAO9Few92",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d02a57f3-436c-4880-c88d-8c937b9acd29"
      },
      "cell_type": "code",
      "source": [
        "word_count_vector.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 124901)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "NVE8C2Hwew94",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's limit our vocabulary size to 10,000"
      ]
    },
    {
      "metadata": {
        "id": "KVXkHxjKew96",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "ce84a1d1-8cd0-4de0-dd8b-402904bc5e86"
      },
      "cell_type": "code",
      "source": [
        "cv=CountVectorizer(max_df=0.85,stop_words=stopwords,max_features=10000)\n",
        "word_count_vector=cv.fit_transform(docs)\n",
        "word_count_vector.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "2dJNTCNzew98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let's look at 10 words from our vocabulary. Sweet, these are mostly programming related."
      ]
    },
    {
      "metadata": {
        "id": "z_1pMoJ3ew99",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "e492d196-d2f7-40ac-84f6-35b543a3640f"
      },
      "cell_type": "code",
      "source": [
        "list(cv.vocabulary_.keys())[:20]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['serializing',\n",
              " 'private',\n",
              " 'struct',\n",
              " 'public',\n",
              " 'class',\n",
              " 'contains',\n",
              " 'properties',\n",
              " 'string',\n",
              " 'serialize',\n",
              " 'attempt',\n",
              " 'stream',\n",
              " 'disk',\n",
              " 'using',\n",
              " 'xmlserializer',\n",
              " 'error',\n",
              " 'saying',\n",
              " 'only',\n",
              " 'types',\n",
              " 'serialized',\n",
              " 'don']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "O_8A9Mbfew-B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also get the vocabulary by using `get_feature_names()`"
      ]
    },
    {
      "metadata": {
        "id": "u_7cS2fTew-C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "57b41c75-9f7a-4ecb-a6e3-563b1d116980"
      },
      "cell_type": "code",
      "source": [
        "list(cv.get_feature_names())[2000:2015]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['customization',\n",
              " 'customize',\n",
              " 'customized',\n",
              " 'customlog',\n",
              " 'customview',\n",
              " 'cut',\n",
              " 'cv',\n",
              " 'cv_',\n",
              " 'cval',\n",
              " 'cvc',\n",
              " 'cw',\n",
              " 'cwd',\n",
              " 'cx',\n",
              " 'cx_oracle',\n",
              " 'cxf']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "n57cjp1rew-F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### TfidfTransformer to Compute Inverse Document Frequency (IDF) \n",
        "In the code below, we are essentially taking the sparse matrix from CountVectorizer to generate the IDF when you invoke `fit`. An extremely important point to note here is that the IDF should be based on a large corpora and should be representative of texts you would be using to extract keywords. I've seen several articles on the Web that compute the IDF using a handful of documents. To understand why IDF should be based on a fairly large collection, please read this [page from Standford's IR book](https://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html)."
      ]
    },
    {
      "metadata": {
        "id": "quzJWdw_ew-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04eff4cd-41a4-4676-ba34-5f7c77373259"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(word_count_vector)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "dQnsDixkew-I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's look at some of the IDF values:"
      ]
    },
    {
      "metadata": {
        "id": "Y9h8WySEew-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ac219849-c88b-411e-ad32-464b23eebdd6"
      },
      "cell_type": "code",
      "source": [
        "tfidf_transformer.idf_"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7.37717703,  9.80492526,  9.51724319, ...,  8.82409601,\n",
              "       10.21039037,  9.51724319])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "Vj8Dd4tzew-L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Computing TF-IDF and Extracting Keywords"
      ]
    },
    {
      "metadata": {
        "id": "dDSNWyO9ew-M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once we have our IDF computed, we are now ready to compute TF-IDF and extract the top keywords. In this example, we will extract top keywords for the questions in `data/stackoverflow-test.json`. This data file has 500 questions with fields identical to that of `data/stackoverflow-data-idf.json` as we saw above. We will start by reading our test file, extracting the necessary fields (title and body) and get the texts into a list."
      ]
    },
    {
      "metadata": {
        "id": "pb90hKe-ew-M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# read test docs into a dataframe and concatenate title and body\n",
        "df_test=pd.read_json(\"/content/gdrive/My Drive/DataScience/data/data/stackoverflow-test.json\",lines=True)\n",
        "df_test['text'] = df_test['title'] + df_test['body']\n",
        "df_test['text'] =df_test['text'].apply(lambda x:pre_process(x))\n",
        "\n",
        "# get test docs into a list\n",
        "docs_test=df_test['text'].tolist()\n",
        "docs_title=df_test['title'].tolist()\n",
        "docs_body=df_test['body'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KQJrcHLcew-O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "    \n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "\n",
        "    for idx, score in sorted_items:\n",
        "        fname = feature_names[idx]\n",
        "        \n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v9rXT3mqew-T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The next step is to compute the tf-idf value for a given document in our test set by invoking `tfidf_transformer.transform(...)`. This generates a vector of tf-idf scores. Next, we sort the words in the vector in descending order of tf-idf values and then iterate over to extract the top-n items with the corresponding feature names, In the example below, we are extracting keywords for the first document in our test set. \n",
        "\n",
        "The `sort_coo(...)` method essentially sorts the values in the vector while preserving the column index. Once you have the column index then its really easy to look-up the corresponding word value as you would see in `extract_topn_from_vector(...)` where we do `feature_vals.append(feature_names[idx])`."
      ]
    },
    {
      "metadata": {
        "id": "6wKhzfWqew-T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "d27d3250-937e-4aee-df72-954e23c54c28"
      },
      "cell_type": "code",
      "source": [
        "# you only needs to do this once\n",
        "feature_names=cv.get_feature_names()\n",
        "\n",
        "# get the document that we want to extract keywords from\n",
        "doc=docs_test[0]\n",
        "\n",
        "#generate tf-idf for the given document\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
        "\n",
        "#sort the tf-idf vectors by descending order of scores\n",
        "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "#extract only the top n; n here is 10\n",
        "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "\n",
        "# now print the results\n",
        "print(\"\\n=====Title=====\")\n",
        "print(docs_title[0])\n",
        "print(\"\\n=====Body=====\")\n",
        "print(docs_body[0])\n",
        "print(\"\\n===Keywords===\")\n",
        "for k in keywords:\n",
        "    print(k,keywords[k])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=====Title=====\n",
            "Integrate War-Plugin for m2eclipse into Eclipse Project\n",
            "\n",
            "=====Body=====\n",
            "<p>I set up a small web project with JSF and Maven. Now I want to deploy on a Tomcat server. Is there a possibility to automate that like a button in Eclipse that automatically deploys the project to Tomcat?</p>\n",
            "\n",
            "<p>I read about a the <a href=\"http://maven.apache.org/plugins/maven-war-plugin/\" rel=\"nofollow noreferrer\">Maven War Plugin</a> but I couldn't find a tutorial how to integrate that into my process (eclipse/m2eclipse).</p>\n",
            "\n",
            "<p>Can you link me to help or try to explain it. Thanks.</p>\n",
            "\n",
            "===Keywords===\n",
            "eclipse 0.593\n",
            "war 0.317\n",
            "integrate 0.281\n",
            "maven 0.273\n",
            "tomcat 0.27\n",
            "project 0.239\n",
            "plugin 0.214\n",
            "automate 0.157\n",
            "jsf 0.152\n",
            "possibility 0.146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d1G-OWxVew-W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From the keywords above, the top keywords actually make sense, it talks about `eclipse`, `maven`, `integrate`, `war` and `tomcat` which are all unique to this specific question. There are a couple of kewyords that could have been eliminated such as `possibility` and perhaps even `project` and you can do this by adding more common words to your stop list and you can even create your own set of stop list, very specific to your domain as [described here](http://kavita-ganesan.com/tips-for-constructing-custom-stop-word-lists/).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "v27I_dTjew-X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put the common code into several methods\n",
        "def get_keywords(idx):\n",
        "\n",
        "    #generate tf-idf for the given document\n",
        "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs_test[idx]]))\n",
        "\n",
        "    #sort the tf-idf vectors by descending order of scores\n",
        "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "    #extract only the top n; n here is 10\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "    \n",
        "    return keywords\n",
        "\n",
        "def print_results(idx,keywords):\n",
        "    # now print the results\n",
        "    print(\"\\n=====Title=====\")\n",
        "    print(docs_title[idx])\n",
        "    print(\"\\n=====Body=====\")\n",
        "    print(docs_body[idx])\n",
        "    print(\"\\n===Keywords===\")\n",
        "    for k in keywords:\n",
        "        print(k,keywords[k])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1vhNX-uiew-Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's look at keywords generated for a much longer question: \n"
      ]
    },
    {
      "metadata": {
        "id": "uOTq-Slfew-a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "32e9e6e0-0af9-4385-cb30-abb1f71536c5"
      },
      "cell_type": "code",
      "source": [
        "idx=120\n",
        "keywords=get_keywords(idx)\n",
        "print_results(idx,keywords)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=====Title=====\n",
            "SQL Import Wizard - Error\n",
            "\n",
            "=====Body=====\n",
            "<p>I have a CSV file that I'm trying to import into SQL Management Server Studio.</p>\n",
            "\n",
            "<p>In Excel, the column giving me trouble looks like this:\n",
            "<a href=\"https://i.stack.imgur.com/pm0uS.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/pm0uS.png\" alt=\"enter image description here\"></a></p>\n",
            "\n",
            "<p>Tasks > import data > Flat Source File > select file</p>\n",
            "\n",
            "<p><a href=\"https://i.stack.imgur.com/G4b6I.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/G4b6I.png\" alt=\"enter image description here\"></a></p>\n",
            "\n",
            "<p>I set the data type for this column to DT_NUMERIC, adjust the DataScale to 2 in order to get 2 decimal places, but when I click over to Preview, I see that it's clearly not recognizing the numbers appropriately:</p>\n",
            "\n",
            "<p><a href=\"https://i.stack.imgur.com/NZhiQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/NZhiQ.png\" alt=\"enter image description here\"></a></p>\n",
            "\n",
            "<p>The column mapping for this column is set to type = decimal; precision 18; scale 2.</p>\n",
            "\n",
            "<p>Error message: Data Flow Task 1: Data conversion failed. The data conversion for column \"Amount\" returned status value 2 and status text \"The value could not be converted because of a potential loss of data.\".\n",
            " (SQL Server Import and Export Wizard)</p>\n",
            "\n",
            "<p>Can someone identify where I'm going wrong here?  Thanks!</p>\n",
            "\n",
            "===Keywords===\n",
            "column 0.365\n",
            "import 0.286\n",
            "data 0.283\n",
            "wizard 0.27\n",
            "decimal 0.227\n",
            "conversion 0.224\n",
            "sql 0.217\n",
            "status 0.164\n",
            "file 0.147\n",
            "appropriately 0.142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oYodp2l8ew-b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Whoala! Now you can extract important keywords from any type of text! "
      ]
    }
  ]
}